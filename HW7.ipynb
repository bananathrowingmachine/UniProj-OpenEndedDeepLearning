{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w_R6GCqTCkVo",
        "outputId": "6da5fd4f-2996-4c9c-a755-9ce70d2c9c2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "#@title Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set device (use GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtVVL_TuCkVo"
      },
      "source": [
        "# Part 1: Load/Analyze Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbvdzOIUbJY4"
      },
      "source": [
        "##What is your dataset? Please describe your dataset and where you got it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oFYpS05bg8s"
      },
      "source": [
        "My dataset is from https://github.com/bananathrowingmachine/FastPartitionExperimentDocs/tree/main/Previous%20Results which is some saved data generated from a personal project a wrote in May, which I made just to see if I could make a really fast algorith that solves the psuedo-polynomial time partition problem, at least compared to the solution found on Wikipedia (which I did infact beat quite well). The data essentially has 3 x variables and a y variable, where the x variables are the algorithm type (with there being 4 types, the Wikipedia solution, one change I made, another change I made, and then both changes combined together), the amount of integers in the set, and the approximate sum of the set and then the y variable is the average amount of iterations each method took to solve the same randomly(-ish) generated set. For more information on how the experiment was run as a whole you can look here: https://github.com/bananathrowingmachine/FastPartitionExperiment\n",
        "\n",
        "For this project the data is known as \"Data1.xlsx\" for the data stored when I ran the experiment in late May, and \"Data2.xlsx\" for the data stored when I ran the experiment in late July, and will be included with whatever folders this notebook is inside of.\n",
        "\n",
        "How I convert the data is I will turn every combination of inputs and results into a single long list, where the first variable will denote if the data comes from a Memoized algorithm (signified as 1) or a Tabulated algorithm (signified as 0), the second variable will denote if the date comes from a \"Crazy\" algorithm (signified as 1, with the main GitHub repo elaborating on what \"Crazy\" means), or a \"Normal\" algorithm (signified as 0), the third variable will be the integer count, and the fourth variable will be the \"Sum Target\" (also elaborated on in the main GitHub repository)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2THiVIuNbmkl"
      },
      "source": [
        "##Load your dataset / Implement your dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZuivL4ab_e4"
      },
      "outputs": [],
      "source": [
        "sheetData = pd.ExcelFile('Data.xlsx')\n",
        "\n",
        "# TODO - if not already done, please make a train/val/test split of your dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR15EWtpcGDL"
      },
      "source": [
        "##Dataset Analyis Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxFU7WRScJTg"
      },
      "outputs": [],
      "source": [
        "# TODO - code for collecting statistics from your dataset / plots and analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk8kMwR-cNL9"
      },
      "source": [
        "TODO - Provide the analysis and statistics of your data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EmxVzkHhtvo"
      },
      "source": [
        "# Part 2: Literature Review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBxv3M_RcUIh"
      },
      "source": [
        "TODO - Follow instructions in the assignment for your literature review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKQ_C71ThwqD"
      },
      "source": [
        "# Part 3: Implement networks in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5JKyDsNh9CP"
      },
      "source": [
        "Note of warning here: Depending on how easily you can allocate GPU resources, you may want to make your network much shallower so that you can train it more easily\n",
        "Aim to have one gradient update take no more than a few seconds\n",
        "May also want to reduce the number of training steps if training is too slow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le5pnYqaclJU"
      },
      "outputs": [],
      "source": [
        "# TODO - implement your PyTorch Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72d14EfbcoCe"
      },
      "outputs": [],
      "source": [
        "# TODO - implement your training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ1m877lcqa0"
      },
      "outputs": [],
      "source": [
        "# TODO - plot train and validation accuracy of your model during training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj7BrJ6wcvI4"
      },
      "source": [
        "Hint: look at earlier homeworks / resuse code from those to help you here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpqXry-Sh2O4"
      },
      "source": [
        "# Part 4: Run Hyperparameter Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UO4tkRhc6sv"
      },
      "outputs": [],
      "source": [
        "# TODO - perform hyperparameter grid searches and plot accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esd0YWZQc9uv"
      },
      "outputs": [],
      "source": [
        "# TODO - perform other experiments and plot accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "risBZpbLdImE"
      },
      "outputs": [],
      "source": [
        "# TODO - calculate the final test accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVXiqisoiR_r"
      },
      "source": [
        "TODO - Explain your experiments above. See assignment doc for more details."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}