{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w_R6GCqTCkVo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1772db20-6cbd-4da9-a13f-1b10b9d87bd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "#@title Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set device (use GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtVVL_TuCkVo"
      },
      "source": [
        "# Part 1: Load/Analyze Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbvdzOIUbJY4"
      },
      "source": [
        "##What is your dataset? Please describe your dataset and where you got it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oFYpS05bg8s"
      },
      "source": [
        "My dataset is from https://github.com/bananathrowingmachine/FastPartitionExperimentDocs/tree/main/Previous%20Results which is some saved data generated from a personal project a wrote in May, which I made just to see if I could make a really fast algorith that solves the psuedo-polynomial time partition problem, at least compared to the solution found on Wikipedia (which I did infact beat quite well). The data essentially has 3 x variables and a y variable, where the x variables are the algorithm type (with there being 4 types, the Wikipedia solution, one change I made, another change I made, and then both changes combined together), the amount of integers in the set, and the approximate sum of the set and then the y variable is the average amount of iterations each method took to solve the same randomly(-ish) generated set. For more information on how the experiment was run as a whole you can look here: https://github.com/bananathrowingmachine/FastPartitionExperiment\n",
        "\n",
        "For this project the data is just linked directly to where it's stored on GitHub with the argument raw=true so that it doesn't redirect to the human visiable page.\n",
        "\n",
        "How I convert the data is I will turn every combination of inputs and results into a single long list, where the first variable will denote if the data comes from a Memoized algorithm (signified as 1) or a Tabulated algorithm (signified as 0), the second variable will denote if the date comes from a \"Crazy\" algorithm (signified as 1, with the main GitHub repo elaborating on what \"Crazy\" means), or a \"Normal\" algorithm (signified as 0), the third variable will be the \"Sum Target\" (also elaborated on in the main GitHub repository) and the fourth variable will be the integer count.\n",
        "\n",
        "Due to my data being available publicly on my Github, the direct links to the sheets already in the code are sufficient for accessing the necessary data frames and vstack them all into a numpy array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2THiVIuNbmkl"
      },
      "source": [
        "##Load your dataset / Implement your dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rZuivL4ab_e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2294f44-fdf0-4337-87d7-80e4746736ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (2688, 4)\n",
            "y_train shape: (2688,)\n",
            "x_test shape: (672, 4)\n",
            "y_test shape: (672,)\n"
          ]
        }
      ],
      "source": [
        "def processSheet(fileName: str):\n",
        "    sheet = pd.ExcelFile(fileName)\n",
        "    processedData = []\n",
        "    for sheetName in sheet.sheet_names:\n",
        "        if sheetName not in ['Recursive Normal', 'Absolute Target Sum']:\n",
        "            df = pd.read_excel(sheet, sheet_name=sheetName)\n",
        "            dfReset = df.reset_index().rename(columns={'index': 'Row_Index'})\n",
        "            dfLong = dfReset.melt(id_vars=['Row_Index'], var_name='Col_Index', value_name='Value')\n",
        "            dfCleaned = dfLong[dfLong['Col_Index'] != 'Unnamed: 0'].copy()\n",
        "            dfCleaned['Col_Index'] = pd.to_numeric(dfCleaned['Col_Index']).astype(int)\n",
        "\n",
        "            rows = dfCleaned.values.shape[0]\n",
        "            crazyVals = np.ones((rows, 1)) if 'Crazy' in sheetName else np.zeros((rows, 1))\n",
        "            memoVals = np.ones((rows, 1)) if 'Memoized' in sheetName else np.zeros((rows, 1))\n",
        "            processedData.append(np.hstack((memoVals, crazyVals, dfCleaned)))\n",
        "\n",
        "    return np.vstack(processedData)\n",
        "\n",
        "# Links to the excel sheets directly on my GitHub so I don't have to upload them to Colab.\n",
        "# These files are public so the data will be converted into a dataframe automatically when this cell is run\n",
        "data1 = 'https://github.com/bananathrowingmachine/FastPartitionExperimentDocs/blob/main/Previous%20Results/May%2026%2C%202025/data_tables/Results.xlsx?raw=true'\n",
        "data2 = 'https://github.com/bananathrowingmachine/FastPartitionExperimentDocs/blob/main/Previous%20Results/Jul%2012%2C%202025/data_tables/Results.xlsx?raw=true'\n",
        "numpyDataArray = np.vstack([processSheet(data1), processSheet(data2)])\n",
        "x_train, x_test, y_train, y_test = train_test_split(numpyDataArray[:, 0:4], numpyDataArray[:, 4], test_size=0.2, random_state=42)\n",
        "print(f'x_train shape: {x_train.shape}')\n",
        "print(f'y_train shape: {y_train.shape}')\n",
        "print(f'x_test shape: {x_test.shape}')\n",
        "print(f'y_test shape: {y_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR15EWtpcGDL"
      },
      "source": [
        "##Dataset Analyis Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxFU7WRScJTg"
      },
      "outputs": [],
      "source": [
        "# TODO - code for collecting statistics from your dataset / plots and analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk8kMwR-cNL9"
      },
      "source": [
        "TODO - Provide the analysis and statistics of your data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EmxVzkHhtvo"
      },
      "source": [
        "# Part 2: Literature Review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBxv3M_RcUIh"
      },
      "source": [
        "TODO - Follow instructions in the assignment for your literature review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKQ_C71ThwqD"
      },
      "source": [
        "# Part 3: Implement networks in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5JKyDsNh9CP"
      },
      "source": [
        "Note of warning here: Depending on how easily you can allocate GPU resources, you may want to make your network much shallower so that you can train it more easily\n",
        "Aim to have one gradient update take no more than a few seconds\n",
        "May also want to reduce the number of training steps if training is too slow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le5pnYqaclJU"
      },
      "outputs": [],
      "source": [
        "# TODO - implement your PyTorch Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72d14EfbcoCe"
      },
      "outputs": [],
      "source": [
        "# TODO - implement your training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ1m877lcqa0"
      },
      "outputs": [],
      "source": [
        "# TODO - plot train and validation accuracy of your model during training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj7BrJ6wcvI4"
      },
      "source": [
        "Hint: look at earlier homeworks / resuse code from those to help you here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpqXry-Sh2O4"
      },
      "source": [
        "# Part 4: Run Hyperparameter Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UO4tkRhc6sv"
      },
      "outputs": [],
      "source": [
        "# TODO - perform hyperparameter grid searches and plot accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esd0YWZQc9uv"
      },
      "outputs": [],
      "source": [
        "# TODO - perform other experiments and plot accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "risBZpbLdImE"
      },
      "outputs": [],
      "source": [
        "# TODO - calculate the final test accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVXiqisoiR_r"
      },
      "source": [
        "TODO - Explain your experiments above. See assignment doc for more details."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}